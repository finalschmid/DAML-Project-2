{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a233ee62",
   "metadata": {},
   "source": [
    "# DAML Project 2 - Callum Smith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a7521a",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Convert the csv file into an appropriate format for our neural networks\n",
    "\n",
    "1. Create variables where you count the number of electrons, photons, muons, jets and bjets in the event (ignore charge)\n",
    "    * each line in csv is a single event, which objects `obj` separated by semicolons:  \n",
    "    `event ID; process ID; event weight; MET; METphi; obj1, E1, pt1, eta1, phi1; obj2, E2, pt2, eta2, phi2; ...`<br>\n",
    "    * e.g. row 1 of the `SM` dataset looks like:    \n",
    "    `5702564; z_jets; 1; 102549; -2.9662; j,335587,132261,-1.57823,1.02902; j,107341,106680,-0.0989776, -2.67901; j,85720.1,62009,0.840127,-1.73805; j,270540,58844.5,2.20566,1.6064; j,55173.9,52433.5,-0.183147,2.62501; j,48698.6,37306.4,-0.719927,-1.7898; j,148467,23648,-2.52332,-1.70799; e-,186937,131480,0.888915,-0.185666; e+,80014.3,79281.7,0.135844,0.275231;`<br>\n",
    "    * `obj`: the object type:\n",
    "\n",
    "        |Key|Particle|\n",
    "        |---|---|\n",
    "        |j|jet|\n",
    "        |b|b-jet|\n",
    "        |e-|electron|\n",
    "        |e+|positron|\n",
    "        |m-|muon|\n",
    "        |m+|muon+|\n",
    "        |g|photon|\n",
    "\n",
    "    * for each event e.g. electron = number of electrons/positrons (ignore charge) in that event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a1c03f",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac92fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # numerical computing with arrays\n",
    "import pandas as pd # data manipulation and analysis\n",
    "from sklearn.preprocessing import StandardScaler # normalize features to mean=0, std=1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00bfc6",
   "metadata": {},
   "source": [
    "Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "857cedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first 5 columns before objects\n",
    "base_cols = [\"event_id\", \"process_id\", \"event_weight\", \"MET\", \"METphi\"] \n",
    "\n",
    "\n",
    "# read our data file\n",
    "def read(path):  # file name as argument\n",
    "\n",
    "    # Count semicolons per line to get max number of fields (semicolons + 1)\n",
    "    max_cols = max(line.count(\";\") for line in open(path, encoding=\"utf-8\")) + 1 # unsure of the +1 here...\n",
    "    names = base_cols + [f\"obj{i+1}\" for i in range(max_cols - len(base_cols))] # full list of column names separated by ;\n",
    "    df = pd.read_csv(path, sep=\";\", header=None, names=names, engine=\"python\", dtype=str) # read data separated by ; as strings\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c3f919",
   "metadata": {},
   "source": [
    "Convert columns which are numbers to integers or floats: `event ID`, `event weight`, `MET`, `METphi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b1f1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert number columns from strings to integers or floats\n",
    "def numeric(df):\n",
    "\n",
    "    num_cols = [\"event_id\", \"event_weight\", \"MET\", \"METphi\"]\n",
    "    df[num_cols] = df[num_cols].apply(pd.to_numeric, errors=\"coerce\") # float64 or int64 depending on the data supplied\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bc775",
   "metadata": {},
   "source": [
    "Count the number of electrons, photons, muons, jets and bjets in the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f65c03fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# count particles\n",
    "def particle_count(df):\n",
    "\n",
    "    obj_keys = [\"j\", \"b\", \"e-\", \"e+\", \"m-\", \"m+\", \"g\"] # keys for each object type\n",
    "    obj_cols = df.columns[len(base_cols):] # take columns after the first 5, i.e [5:]\n",
    "\n",
    "    # Extract the object type from every object cell\n",
    "    types = df[obj_cols].map(lambda s: str(s).split(\",\", 1)[0].strip() if pd.notna(s) and str(s).strip() else None) # lambda applied to every cell if condition met\n",
    "    \"\"\"\n",
    "        If the cell is not NaN and not an empty string, convert to str, split on the first comma, take the token before it (the object code), and strip whitespace.\n",
    "        Otherwise return None (for missing slots) \n",
    "    \"\"\"\n",
    "\n",
    "    # Count per row how many times each type appears\n",
    "    counts = pd.DataFrame({i: (types == i).sum(axis=1) for i in obj_keys}).astype(\"int64\")\n",
    "    \"\"\"\n",
    "        types == k creates a boolean DataFrame (True where the cell equals the object k).\n",
    "        .sum(axis=1) counts True per row (because True=1, False=0), i.e., how many times k appears in that event across all object slots.\n",
    "        The dict comprehension builds a column per object type.\n",
    "        pd.DataFrame(...) turns that dict into a DataFrame with columns in obj_keys order.\n",
    "        .astype(\"int64\") casts boolean sums to integer counts\n",
    "    \"\"\"\n",
    "\n",
    "    # columns for total electrons and total muons, regardless of charge\n",
    "    counts[\"e\"] = counts[\"e-\"] + counts[\"e+\"]\n",
    "    counts[\"m\"] = counts[\"m-\"] + counts[\"m+\"]\n",
    "\n",
    "\n",
    "    # reorder columns: |N ele| N muon| N jets| N bjets| N photons|\n",
    "    counts = (counts\n",
    "            .drop(columns=[\"e-\", \"e+\", \"m-\", \"m+\"]) # get rid of charged columns\n",
    "            .reindex(columns=[\"e\",\"m\",\"j\", \"b\", \"g\"], fill_value=0)) # reorder remaining columns; fill_value=0 fills empty columns with 0's not NaNs\n",
    "\n",
    "\n",
    "    # add these columns to the overall dataframe as integers\n",
    "    df = pd.concat([counts, df], axis=1)\n",
    "    df[[\"e\",\"m\",\"j\",\"b\",\"g\"]] = df[[\"e\",\"m\",\"j\",\"b\",\"g\"]].astype(\"int64\") # convert new columns to integers\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e122d56",
   "metadata": {},
   "source": [
    "3. **Sort by energy** (largest to smallest)\n",
    "    * remember the main df is ordered as follows:  \n",
    "    `event ID; process ID; event weight; MET; METphi; obj1, E1, pt1, eta1, phi1; obj2, E2, pt2, eta2, phi2; ...`<br>\n",
    "2. Choose an appropiate number of particles to study per event (recommended: **8**)  \n",
    "\n",
    "4. If the event has more than 8 particles choose the **8 particles** with **highest energy and truncate** the rest\n",
    "    * objects are already in order of decreasing energy\n",
    "    * just take the first 8 objects and drop the rest\n",
    "5. take logarithm of MET, energy and momentum variables\n",
    "6. If the event has less than 8 particles, create kinematic variables with 0 values for the missing particles...\n",
    "    * replace NaN with 0\n",
    "\n",
    "- take sine and cosine of METphi and phiN for later standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59a61922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# truncate and mask\n",
    "def trunc_mask(df):\n",
    "\n",
    "    # Vectorized NumPy/pandas operations - Operates on entire columns at once using optimized C code\n",
    "    base_cols = [i for i in df.columns[:10]]\n",
    "    obj_cols  = sorted([c for c in df.columns if c.startswith(\"obj\")],\n",
    "                    key=lambda s: int(s[3:])) # is the key necessary?\n",
    "\n",
    "\n",
    "    # Wide -> long and split into fields\n",
    "    S = df[obj_cols].stack() # (row, obj_col)\n",
    "    parts = S.str.split(\",\", n=4, expand=True).apply(lambda c: c.str.strip())\n",
    "    parts.columns = [\"type\",\"E\",\"pt\",\"eta\",\"phi\"]\n",
    "\n",
    "\n",
    "    # Numeric conversion for object fields\n",
    "    for i in [\"E\",\"pt\",\"eta\",\"phi\"]:\n",
    "        parts[i] = pd.to_numeric(parts[i], errors=\"coerce\")\n",
    "\n",
    "\n",
    "    # Add event (row) index and sort by E within each event - FOR ORDERING??\n",
    "    parts.index = parts.index.set_names([\"row\",\"slot\"])\n",
    "    long = parts.reset_index().sort_values([\"row\",\"E\"], ascending=[True, False])\n",
    "\n",
    "\n",
    "    # Keep top 8 per event\n",
    "    long[\"rank\"] = long.groupby(\"row\").cumcount() + 1\n",
    "    topK = long[long[\"rank\"] <= 8].copy()\n",
    "\n",
    "\n",
    "    # calculate logE and logpt \n",
    "    eps = 1e-12 # so we have no divergence log(0)\n",
    "    topK[\"logE\"]  = np.log(np.clip(topK[\"E\"],  eps, None))\n",
    "    topK[\"logpt\"] = np.log(np.clip(topK[\"pt\"], eps, None))\n",
    "\n",
    "\n",
    "    # calculate sinphi and cosphi\n",
    "    topK[\"sinphi\"] = np.sin(topK[\"phi\"])\n",
    "    topK[\"cosphi\"] = np.cos(topK[\"phi\"])\n",
    "\n",
    "\n",
    "    # Pivot back to wide numeric columns (type1, logE1, logpt1, ...)\n",
    "    wide = (topK\n",
    "            .set_index([\"row\",\"rank\"])[[\"type\", \"logE\", \"logpt\", \"eta\", \"sinphi\", \"cosphi\"]]\n",
    "            .unstack(\"rank\"))\n",
    "\n",
    "\n",
    "    # Flatten columns like ('logE', 1) -> 'logE1'\n",
    "    wide.columns = [f\"{f}{k}\" for f, k in wide.columns]\n",
    "\n",
    "\n",
    "    # Order columns per slot: type, logE, logpt, eta, sinphi, cosphi\n",
    "    ordered = []\n",
    "    for i in range(1, 9):\n",
    "        ordered += [f\"type{i}\", f\"logE{i}\", f\"logpt{i}\", f\"eta{i}\", f\"sinphi{i}\", f\"cosphi{i}\"]\n",
    "    wide = wide.reindex(columns=[c for c in ordered if c in wide.columns])\n",
    "\n",
    "\n",
    "    # Merge back base/meta columns\n",
    "    df = (df.reset_index()\n",
    "        .rename(columns={\"index\": \"row\"})[[\"row\"] + base_cols]\n",
    "        .merge(wide.reset_index(), on=\"row\", how=\"left\")\n",
    "        .drop(columns=\"row\"))\n",
    "\n",
    "\n",
    "    # calculate log(MET)\n",
    "    met_num = pd.to_numeric(df[\"MET\"], errors=\"coerce\")\n",
    "    df[\"MET\"] = np.log(np.clip(met_num, eps, None))\n",
    "    df.rename(columns={\"MET\": \"logMET\"}, inplace=True) # rename column\n",
    "\n",
    "\n",
    "    # calculate sin(METphi) and cos(METphi)\n",
    "    metphi_num = pd.to_numeric(df[\"METphi\"], errors=\"coerce\")\n",
    "    df.drop(columns=[\"METphi\"], inplace=True) # get rid of METphi\n",
    "    df.insert(df.columns.get_loc(\"type1\"),     \"sin_METphi\", np.sin(metphi_num)) # replace with sin(METphi)\n",
    "    df.insert(df.columns.get_loc(\"type1\"), \"cos_METphi\", np.cos(metphi_num)) # and cos(METphi)\n",
    "\n",
    "\n",
    "    # Convert NaN's in object columns to 0's \n",
    "    df = df.fillna(0)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a67f0cd",
   "metadata": {},
   "source": [
    "## TEMP PROGRESS REPORT\n",
    "- the ordering of training_top8 is now: `e; m; j; b; g; event ID; process ID; event weight; MET; METphi; obj1; obj2; obj3; obj4; obj5; obj6; obj7; obj8;`<br>\n",
    "where for each object we have `obji, log(Ei), log(pti), etai, phii`<br>  \n",
    "\n",
    "    * ~~still need to take the log of MET: log(MET) for each event~~ __done__\n",
    "    * ~~maybe go back and convert all columns which are numbers to floats instead of switching back and forth to strings~~ __done__\n",
    "    * ~~maybe don't use fields for each object (also what is a field here?) but also were given in ;,,,,,; format...~~ __done__\n",
    "    * ~~what is \"`MinMaxScalar` or similar\" and what does it mean to standardise the variables?~~ __use `StandardScaler` instead__\n",
    "\n",
    "- ordering now: `e; m; j; b; g; event ID; process ID; event weight; log(MET); sin(METphi); cos(METphi); obj1; log(E1); log(pt1); eta1; sin(phi1); cos(phi1) ... `<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e86a8",
   "metadata": {},
   "source": [
    "Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "517d8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an overall function for preprocessing\n",
    "def pre_process(file):\n",
    "\n",
    "    data = read(file) # read background file and make it a dataset\n",
    "    numeric(data) #convert number columns to floats\n",
    "    data = particle_count(data) # count particles\n",
    "    data = trunc_mask(data) # truncate and mask\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "test = pre_process(\"background_chan2b_7.8.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43f3f3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'm', 'j', 'b', 'g', 'event_id', 'process_id', 'event_weight', 'logMET', 'sin_METphi', 'cos_METphi', 'type1', 'logE1', 'logpt1', 'eta1', 'sinphi1', 'cosphi1', 'type2', 'logE2', 'logpt2', 'eta2', 'sinphi2', 'cosphi2', 'type3', 'logE3', 'logpt3', 'eta3', 'sinphi3', 'cosphi3', 'type4', 'logE4', 'logpt4', 'eta4', 'sinphi4', 'cosphi4', 'type5', 'logE5', 'logpt5', 'eta5', 'sinphi5', 'cosphi5', 'type6', 'logE6', 'logpt6', 'eta6', 'sinphi6', 'cosphi6', 'type7', 'logE7', 'logpt7', 'eta7', 'sinphi7', 'cosphi7', 'type8', 'logE8', 'logpt8', 'eta8', 'sinphi8', 'cosphi8']\n",
      "\n",
      "        type6      logE6     logpt6      eta6   sinphi6   cosphi6\n",
      "0          j  11.358843  11.035035  0.840127 -0.986046 -0.166475\n",
      "1          0   0.000000   0.000000  0.000000  0.000000  0.000000\n",
      "2          0   0.000000   0.000000  0.000000  0.000000  0.000000\n",
      "3          0   0.000000   0.000000  0.000000  0.000000  0.000000\n",
      "4          0   0.000000   0.000000  0.000000  0.000000  0.000000\n",
      "...      ...        ...        ...       ...       ...       ...\n",
      "340263     0   0.000000   0.000000  0.000000  0.000000  0.000000\n",
      "340264     0   0.000000   0.000000  0.000000  0.000000  0.000000\n",
      "340265     j  11.095569  10.254683 -1.479520 -0.035121  0.999383\n",
      "340266     0   0.000000   0.000000  0.000000  0.000000  0.000000\n",
      "340267     0   0.000000   0.000000  0.000000  0.000000  0.000000\n",
      "\n",
      "[340268 rows x 6 columns]\n",
      "\n",
      "   type2      logE2     logpt2      eta2   sinphi2   cosphi2\n",
      "0     j  12.508175  10.982654  2.205660  0.999366 -0.035596\n",
      "1    m-  11.671970  11.568530 -0.462717  0.902712 -0.430245\n",
      "2    m+  11.022931  10.569058 -1.026190  0.409269 -0.912414\n",
      "3     j  12.303644  11.589998  1.333050  0.957282 -0.289155\n",
      "4     b  11.622990  10.144502  2.156020  0.604408 -0.796675\n"
     ]
    }
   ],
   "source": [
    "print([c for c in test.columns]) \n",
    "print(\"\\n\", test.loc[-5:, \"type6\":\"cosphi6\"])\n",
    "print(\"\\n\", test.iloc[:, 17:23].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a9bbe1",
   "metadata": {},
   "source": [
    "8. After the dataset is ready, use `MinMaxScalar` or similar to standardise the training variables over the SM dataset\n",
    "    * Scaling and normalizing features to improve model performance and training stability\n",
    "    * i.e. normalisation; giving all data points the same scale\n",
    "    * “Mahalanobis distance”: $$z = (x - \\mu)^T{\\Sigma^{-1}}$$\n",
    "    * ~~standardise counts: `e; m; j; b; g`~~ maybe not...\n",
    "    * convert `METphi` to `sin(METphi); cos(METphi)` and `phiN` to `sin(phiN); cos(phiN)`\n",
    "    * \"train on the event level variables `MET; METphi` and the kinematics of the particle level objects `log(EN); log(ptN); etaN; phiN`\"\n",
    "    * make the standardisation process a class ~~(for some reason...)~~ \"*Do not recalculate the standardisation*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcb5818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardise the training dataset\n",
    "training = test.copy()\n",
    "train_cols = [\"logMET\", \"sin_METphi\", \"cos_METphi\"] + [f\"{j}{i}\" for j in (\"logE\", \"logpt\", \"eta\", \"sinphi\", \"cosphi\") for i in range(1, 9)] # columns to be standardised\n",
    "training[train_cols] = training[train_cols].astype(\"float64\") # convert training columns to floats before scaling\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "training.loc[:, train_cols] = scaler.fit_transform(training[train_cols]) # Replace in place\n",
    "#training = scaler.fit_transform(df[train_cols])  # standardised training dataset\n",
    "#X_test_num  = scaler.transform(X_test[num_cols])       # reuse fitted scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5abced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        e  m  j  b  g  event_id process_id  event_weight     logMET  \\\n",
      "0       2  0  7  0  0   5702564     z_jets             1  11.538096   \n",
      "1       0  2  1  0  0  13085335     z_jets             1  11.547018   \n",
      "2       0  2  1  1  0     74025    wtopbar             1  11.770725   \n",
      "3       0  2  2  0  0   2419445     z_jets             1  11.261565   \n",
      "4       0  2  1  1  0     43639       wtop             1  11.581994   \n",
      "...    .. .. .. .. ..       ...        ...           ...        ...   \n",
      "340263  0  2  1  1  0        30      ttbar             1  11.092509   \n",
      "340264  0  2  2  1  0       111      ttbar             1  10.980708   \n",
      "340265  0  2  4  1  0        75      ttbar             1  12.744695   \n",
      "340266  0  2  3  0  0  15181306     z_jets             1  12.417140   \n",
      "340267  0  2  3  0  0  19552503     z_jets             1  11.599653   \n",
      "\n",
      "        sin_METphi  ...     logpt7      eta7   sinphi7   cosphi7  type8  \\\n",
      "0        -0.174495  ...  11.280763  0.135844  0.271769  0.962362      j   \n",
      "1         0.924477  ...   0.000000  0.000000  0.000000  0.000000      0   \n",
      "2        -0.924183  ...   0.000000  0.000000  0.000000  0.000000      0   \n",
      "3        -0.887416  ...   0.000000  0.000000  0.000000  0.000000      0   \n",
      "4        -0.855450  ...   0.000000  0.000000  0.000000  0.000000      0   \n",
      "...            ...  ...        ...       ...       ...       ...    ...   \n",
      "340263   -0.914229  ...   0.000000  0.000000  0.000000  0.000000      0   \n",
      "340264    0.505334  ...   0.000000  0.000000  0.000000  0.000000      0   \n",
      "340265    0.720551  ...  10.615726  0.674197 -1.000000 -0.000924      0   \n",
      "340266   -0.750885  ...   0.000000  0.000000  0.000000  0.000000      0   \n",
      "340267    0.996735  ...   0.000000  0.000000  0.000000  0.000000      0   \n",
      "\n",
      "            logE8     logpt8      eta8   sinphi8   cosphi8  \n",
      "0       10.918245  10.867301 -0.183147  0.493912 -0.869512  \n",
      "1        0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "2        0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "3        0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "4        0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "...           ...        ...       ...       ...       ...  \n",
      "340263   0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "340264   0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "340265   0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "340266   0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "340267   0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[340268 rows x 59 columns]\n",
      "        e  m  j  b  g  event_id process_id  event_weight    logMET  \\\n",
      "0       2  0  7  0  0   5702564     z_jets             1  0.198226   \n",
      "1       0  2  1  0  0  13085335     z_jets             1  0.220021   \n",
      "2       0  2  1  1  0     74025    wtopbar             1  0.766519   \n",
      "3       0  2  2  0  0   2419445     z_jets             1 -0.477315   \n",
      "4       0  2  1  1  0     43639       wtop             1  0.305465   \n",
      "...    .. .. .. .. ..       ...        ...           ...       ...   \n",
      "340263  0  2  1  1  0        30      ttbar             1 -0.890304   \n",
      "340264  0  2  2  1  0       111      ttbar             1 -1.163424   \n",
      "340265  0  2  4  1  0        75      ttbar             1  3.145839   \n",
      "340266  0  2  3  0  0  15181306     z_jets             1  2.345650   \n",
      "340267  0  2  3  0  0  19552503     z_jets             1  0.348605   \n",
      "\n",
      "        sin_METphi  ...    logpt7      eta7   sinphi7   cosphi7  type8  \\\n",
      "0        -0.245071  ...  5.090336  0.816775  1.842870  6.527480      j   \n",
      "1         1.309143  ... -0.212942  0.002080 -0.001976  0.000276      0   \n",
      "2        -1.305311  ... -0.212942  0.002080 -0.001976  0.000276      0   \n",
      "3        -1.253315  ... -0.212942  0.002080 -0.001976  0.000276      0   \n",
      "4        -1.208107  ... -0.212942  0.002080 -0.001976  0.000276      0   \n",
      "...            ...  ...       ...       ...       ...       ...    ...   \n",
      "340263   -1.291234  ... -0.212942  0.002080 -0.001976  0.000276      0   \n",
      "340264    0.716372  ... -0.212942  0.002080 -0.001976  0.000276      0   \n",
      "340265    1.020742  ...  4.777691  4.045431 -6.790254 -0.005988      0   \n",
      "340266   -1.060226  ... -0.212942  0.002080 -0.001976  0.000276      0   \n",
      "340267    1.411332  ... -0.212942  0.002080 -0.001976  0.000276      0   \n",
      "\n",
      "           logE8    logpt8      eta8   sinphi8    cosphi8  \n",
      "0       9.050420  9.225477 -2.145262  6.207377 -10.937455  \n",
      "1      -0.113125 -0.113140 -0.000017  0.002083   0.000623  \n",
      "2      -0.113125 -0.113140 -0.000017  0.002083   0.000623  \n",
      "3      -0.113125 -0.113140 -0.000017  0.002083   0.000623  \n",
      "4      -0.113125 -0.113140 -0.000017  0.002083   0.000623  \n",
      "...          ...       ...       ...       ...        ...  \n",
      "340263 -0.113125 -0.113140 -0.000017  0.002083   0.000623  \n",
      "340264 -0.113125 -0.113140 -0.000017  0.002083   0.000623  \n",
      "340265 -0.113125 -0.113140 -0.000017  0.002083   0.000623  \n",
      "340266 -0.113125 -0.113140 -0.000017  0.002083   0.000623  \n",
      "340267 -0.113125 -0.113140 -0.000017  0.002083   0.000623  \n",
      "\n",
      "[340268 rows x 59 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(training)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daml-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
